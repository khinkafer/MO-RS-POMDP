{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa3ad98-bb1a-4fc5-8aa7-ba6cfed6597b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce024cea-08f2-4331-b848-3a18bcfca011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import itertools as it\n",
    "from env import *\n",
    "import pandas as pd\n",
    "import random\n",
    "from Compact_Beurele import *\n",
    "from Compact_MO import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5afc9aa-5af3-46d0-9cb8-800dcb677e51",
   "metadata": {},
   "source": [
    "#### Loading Environment<br>\n",
    "<B>Here:</B> tiger2 and discount=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2324a5a7-c2ee-4b98-9cbb-107b5ba859b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state_change_r2l': 0.1, 'state_change_l2r': 0.1, 'false_observation_get_l_while_r': 0.1, 'false_observation_get_r_while_l': 0.2, 'reward_listen': -0.1, 'reward_low_incorrect': -0.2, 'reward_high_incorrect': -0.4, 'reward_low_correct': 0.2, 'reward_high_correct': 0.4, 'discount_factor': 1, 'initial_wealth': 0}\n"
     ]
    }
   ],
   "source": [
    "e=tiger_POMDP_env(read_config=True,config_address='./tiger2.json',parameters=None)\n",
    "num_to_act=dict(zip(list(e.actions.values()),list(e.actions.keys())))\n",
    "e.discount_factor=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac9a4da-c883-4d01-aa36-a0f06048ca25",
   "metadata": {},
   "source": [
    "## Simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f3021a9-533d-46fc-af77-0593d96689c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth=2                 # maximum depth of planning\n",
    "planning_depth=max_depth    # maximum depth of planning\n",
    "\n",
    "# For Three exponential terms: \n",
    "exp_vorfaktoren=[-1,0.1,2]  # Powers of exponential components\n",
    "exp_weights=[-1,1,2]        # weights of exponential components\n",
    "\n",
    "# Initial beliefs ( same for both methods)\n",
    "initial_Mu=[0.5,0.5]        # initial beliefs about states. \n",
    "theta_0=initial_Mu           # Initial information vectors are same for all exponential terms\n",
    "\n",
    "initial_observation=0       # initial observation before any action\n",
    "initial_wealth=0            # initial amount of accumulated reward at the beginning of the simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5740f2d4-0fcf-4f2c-9d8c-9e536f3ac645",
   "metadata": {},
   "source": [
    "# Beaurle method simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3ebd1f-effd-4461-8105-64dd41bcaa1c",
   "metadata": {},
   "source": [
    "### Optimized Continious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94f381-4a07-4736-84fb-88dc2e9cefb9",
   "metadata": {},
   "source": [
    "set up and creats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33da9ecd-ea8f-4c63-a93f-09cd3eaeb3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############    Setting Environment\n",
    "e.reset()                             # reset the environment\n",
    "initial_observation=e.current_state   # make the initial signal from environment same as what actual \n",
    "                                      #state it has \n",
    "observation=initial_observation       # record the observation\n",
    "state=e.current_state                 # record the state\n",
    "\n",
    "##############      Making Agent\n",
    "# Create the agent\n",
    "ag=Bauerle_Rieder_agent(environment=e ,max_time_step=max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87664162-2ca1-4174-86d3-668d5ecbfc0c",
   "metadata": {},
   "source": [
    "value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ece30d0-ef10-4d96-9c2b-ba2e6e513ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run its value iteration\n",
    "b_S,b_reachables,b_map,b_q_func,b_value_function,b_action_func=ag.continious_optimized_planning(initial_mu_state=initial_Mu,initial_observation=initial_observation,initial_wealth=initial_wealth,\n",
    "    exp_weights=exp_weights,exp_vorfaktoren=exp_vorfaktoren)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e6a137-e89b-4f2d-a2cb-69e86d2fb76f",
   "metadata": {},
   "source": [
    "simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1367b4b4-b5e4-4bfa-b0c3-5e03de77ee53",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t= 0\n",
      "------\n",
      "state: 1 tiger_left\n",
      "last_observation: 1\n",
      "-------------------------------\n",
      "\n",
      "current internal state: (1, (0.5, 0.5), 0)\n",
      "\n",
      "             action: open_right_high\n",
      "             **\n",
      "reward: 0.4 new observation: 1\n",
      "\n",
      "new internal state: (1, (0.25, 0.0, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.25), 1)\n",
      "\n",
      "============================\n",
      "t= 1\n",
      "------\n",
      "state: 1 tiger_left\n",
      "last_observation: 1\n",
      "-------------------------------\n",
      "\n",
      "current internal state: (1, (0.25, 0.0, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.25), 1)\n",
      "\n",
      "             action: open_right_high\n",
      "             **\n",
      "reward: 0.4 new observation: 0\n",
      "\n",
      "new internal state: (0, (0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125), 2)\n",
      "\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "# reset the agent \n",
    "ag.reset(initial_mu_state=initial_Mu,initial_observation=initial_observation,initial_wealth=initial_wealth)\n",
    "\n",
    "#############         Simulation\n",
    "\n",
    "for t in range(planning_depth):\n",
    "    \n",
    "    print('t=',t)\n",
    "    print('------')\n",
    "    print('state:',state,e.states[state])\n",
    "    print('last_observation:',observation)\n",
    "    print('-------------------------------')\n",
    "    print('')\n",
    "    print('current internal state:',ag.current_internal_state)\n",
    "    print('')\n",
    "    \n",
    "    # agent select the action\n",
    "    action,value_of_action,_=ag.do_action()\n",
    "    \n",
    "    #environment feedback\n",
    "    t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "    \n",
    "    # agent update\n",
    "    new_x=ag.update_agent(new_observation=observation) \n",
    "    \n",
    "    print('             action:',num_to_act[action])  \n",
    "    print ('             **' )\n",
    "    print('reward:',reward,'new observation:',observation)\n",
    "    print('')\n",
    "    print('new internal state:', ag.current_internal_state)\n",
    "    print('')\n",
    "    print('============================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa0f7c-2b2c-4237-86b1-967b0f3dfd36",
   "metadata": {},
   "source": [
    "#### Testing make Mu-points function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d03fb70-d845-4826-bf21-13d4eba2c991",
   "metadata": {},
   "source": [
    "<B>Making whole Mu-space and internal states</B><br>\n",
    "These functions have application in discrete part, however in order to assess the size of state space without creating a \"Discrete agent\", they have written in a way that is accessible from Optimized_continious part as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8802641f-5431-4969-8641-327f4cc60a6e",
   "metadata": {},
   "source": [
    "Making Mu-points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68788073-1954-42ef-a2c0-cf45c400dadd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu_space,counts=ag.make_all_mu(partitioning_points_num=5,save_Mu=True, keep_all_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4755e2c4-0d7f-49f1-89c2-dc3721995130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 2002, 142506]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7912078-5c66-4e84-8fa3-555e5e9ef0b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-calculated points\n",
    "f=ag.load_mu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c12ec6-d431-444b-83b6-b5c49fe98d45",
   "metadata": {},
   "source": [
    "Making all internal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d23f5a48-9c4f-4fca-bdd4-3266dbb71b16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "internal_states,counts=ag.make_all_internal_states_CN_discrete(partitioning_points_num=5,save_state_space=True, keep_all_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8661b6e2-3118-4527-8fbd-b3228e1612e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 4004, 285012]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a940d9a7-89c2-4025-a35e-1d2d174dfde7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-calculated states\n",
    "f=ag.load_internal_states_CN_discrete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e26542-c903-4952-a2a9-76e624842ee5",
   "metadata": {},
   "source": [
    "### Discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd7ca1-8a1d-4f73-8b15-0608226a49cd",
   "metadata": {},
   "source": [
    "set up and creats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea68a29c-e2a6-4e39-b3c9-40af638edf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############    Setting Environment\n",
    "e.reset()                             # reset the environment\n",
    "initial_observation=e.current_state   # make the initial signal from environment same as what actual \n",
    "                                      #state it has \n",
    "observation=initial_observation       # record the observation\n",
    "state=e.current_state                 # record the state\n",
    "observations=e.observations.keys()    # fetch the observations\n",
    "exp_num=len(exp_vorfaktoren)          # number of exponential terms\n",
    "num_of_actions=len(e.actions)         # number of actions\n",
    "num_of_observations=len(e.observations) #number of observations \n",
    "\n",
    "################ Making Agent\n",
    "\n",
    "############################## Discrete agent variables \n",
    "max_time_step=max_depth     # depth of planning\n",
    "r_round=5                   # number of decimal digits for rounding\n",
    "partitioning_points_num=24  # number of partitioning points: determines precision of belief estimation\n",
    "\n",
    "# creating the agent: ag2\n",
    "ag2=Bauerle_Rieder_agent(environment=e ,max_time_step=max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4cc28f-9832-478b-846b-a983c915f2c9",
   "metadata": {},
   "source": [
    "value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a659ea43-e2d3-4bbe-9d58-2f40438c1f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run its value iteration\n",
    "b_optimized_mu_size,b_value_func,b_action_func,b_q_func=ag2.discrete_value_iteration(partitioning_points_num,initial_wealth,exp_weights,exp_vorfaktoren,rounding_prec_coeff=100000,\n",
    "                                 r_round=5,save_reachables=True,save_transition_kernel=True,use_buffer=True,\n",
    "                                 memory_threshold=1000000,keep_q_func=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf48f68a-64d0-4ae5-91d9-a33f8ea971a5",
   "metadata": {},
   "source": [
    "simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1044d8d4-cfd9-4a68-948b-7763e3800524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t= 0\n",
      "------\n",
      "state: 0 tiger_right\n",
      "last_observation: 0\n",
      "-------------------------------\n",
      "\n",
      "current internal state: (0, (0.5, 0.5), 0)\n",
      "\n",
      "             action: open_right_high\n",
      "             **\n",
      "reward: -0.4 new observation: 0\n",
      "\n",
      "new internal state: (0, (0.25, 0.0, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.25), 1)\n",
      "\n",
      "============================\n",
      "t= 1\n",
      "------\n",
      "state: 0 tiger_right\n",
      "last_observation: 0\n",
      "-------------------------------\n",
      "\n",
      "current internal state: (0, (0.25, 0.0, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.25), 1)\n",
      "\n",
      "             action: open_right_high\n",
      "             **\n",
      "reward: -0.4 new observation: 0\n",
      "\n",
      "new internal state: (0, (0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125), 2)\n",
      "\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "# reset the agent \n",
    "ag2.reset(initial_mu_state=initial_Mu,initial_observation=initial_observation,initial_wealth=initial_wealth)\n",
    "\n",
    "#############         Simulation\n",
    "\n",
    "for t in range(planning_depth):\n",
    "    \n",
    "    print('t=',t)\n",
    "    print('------')\n",
    "    print('state:',state,e.states[state])\n",
    "    print('last_observation:',observation)\n",
    "    print('-------------------------------')\n",
    "    print('')\n",
    "    print('current internal state:',ag2.current_internal_state)\n",
    "    print('')\n",
    "    \n",
    "    # agent select the action\n",
    "    action,value_of_action,_=ag2.do_action()\n",
    "    \n",
    "    #environment feedback\n",
    "    t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "    \n",
    "    # agent update\n",
    "    new_x=ag2.update_agent(new_observation=observation) \n",
    "    \n",
    "    print('             action:',num_to_act[action])  \n",
    "    print ('             **' )\n",
    "    print('reward:',reward,'new observation:',observation)\n",
    "    print('')\n",
    "    print('new internal state:', ag2.current_internal_state)\n",
    "    print('')\n",
    "    print('============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f4e2a-9792-4d37-971f-6f3dd520c413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b8976dc-6974-442a-b400-58c01b5671db",
   "metadata": {},
   "source": [
    "# Multi-Variate (MO) method simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66535139-4de1-468d-80c6-9cdb4711c65f",
   "metadata": {},
   "source": [
    "### Optimized Continious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc51db8-072f-4075-8902-45328f9c8181",
   "metadata": {},
   "source": [
    "set up and creats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baedb501-8c15-4b15-9f5d-8725306d778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############    Setting Environment\n",
    "e.reset()                             # reset the environment\n",
    "initial_observation=e.current_state   # make the initial signal from environment same as what actual \n",
    "                                      #state it has \n",
    "observation=initial_observation       # record the observation\n",
    "state=e.current_state                 # record the state\n",
    "\n",
    "\n",
    "\n",
    "##############      Making Agent\n",
    "# Create the agent\n",
    "ag=Multi_Variate_agent(environment=e ,max_time_step=max_depth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f24ce9-7e46-4961-a47f-1bfe66f8fa70",
   "metadata": {},
   "source": [
    "value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dec42ebf-89ce-4598-b1f3-4943d2222f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run its value iteration\n",
    "_,b_reachables,b_map,b_q_func,b_value_function,b_action_func=ag.continious_optimized_planning(initial_theta=theta_0,initial_observation=initial_observation,initial_wealth=initial_wealth,\n",
    "    exp_weights=exp_weights,exp_vorfaktoren=exp_vorfaktoren,x_round=5,r_round=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c662f9-03f0-4029-85d1-b1d22a002e31",
   "metadata": {},
   "source": [
    "simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7371db2-2e9f-42d1-aa1b-3cc937f6cdb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t= 0\n",
      "------\n",
      "state: 0 tiger_right\n",
      "last_observation: 0\n",
      "-------------------------------\n",
      "\n",
      "current internal state: ((0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0), (0, 0, 0), 0)\n",
      "\n",
      "             action: open_right_high\n",
      "             **\n",
      "reward: -0.4 new observation: 1\n",
      "\n",
      "new internal state: ((0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1), (-0.07795, 0.008, 0.14538), 1)\n",
      "\n",
      "============================\n",
      "t= 1\n",
      "------\n",
      "state: 1 tiger_left\n",
      "last_observation: 1\n",
      "-------------------------------\n",
      "\n",
      "current internal state: ((0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1), (-0.07795, 0.008, 0.14538), 1)\n",
      "\n",
      "             action: open_right_high\n",
      "             **\n",
      "reward: 0.4 new observation: 1\n",
      "\n",
      "new internal state: ((0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1), (-0.1559, 0.016, 0.29076), 2)\n",
      "\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "# reset the agent \n",
    "ag.reset(theta_0=theta_0,initial_observation=initial_observation,initial_wealth=initial_wealth)\n",
    "\n",
    "#############         Simulation\n",
    "\n",
    "for t in range(planning_depth):\n",
    "    \n",
    "    print('t=',t)\n",
    "    print('------')\n",
    "    print('state:',state,e.states[state])\n",
    "    print('last_observation:',observation)\n",
    "    print('-------------------------------')\n",
    "    print('')\n",
    "    print('current internal state:',ag.current_internal_state)\n",
    "    print('')\n",
    "    \n",
    "    # agent select the action\n",
    "    action,value_of_action,_=ag.do_action()\n",
    "    \n",
    "    #environment feedback\n",
    "    t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "    \n",
    "    # agent update\n",
    "    new_x=ag.update_agent(new_observation=observation) \n",
    "    \n",
    "    print('             action:',num_to_act[action])  \n",
    "    print ('             **' )\n",
    "    print('reward:',reward,'new observation:',observation)\n",
    "    print('')\n",
    "    print('new internal state:', ag.current_internal_state)\n",
    "    print('')\n",
    "    print('============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee2309-4967-4257-8139-c4644e0e10af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84e3f198-7f19-4d35-8f66-4cfe62c53892",
   "metadata": {},
   "source": [
    "#### Testing make internal states function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab050d95-a197-444b-8d2c-de9f6a57cfac",
   "metadata": {},
   "source": [
    "<B>Making whole internal state</B><br>\n",
    "This functions has application in discrete part, however in order to assess the size of state space without creating a \"Discrete agent\", It has written in a way that is accessible from Optimized_continious part as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "694a7f5f-d8ba-4afe-b483-f4524965c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "ag=Multi_Variate_agent(environment=e ,max_time_step=max_depth)\n",
    "internal_states,counts=ag.make_all_internal_states(max_depth,10,initial_wealth,exp_weights,exp_vorfaktoren,rounding_prec_coeff=100000,r_round=3,save_space=True, keep_all_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c58943-a059-47df-8e36-2b6dfc468acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loading func\n",
    "internal_state=ag.load_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5755dc-f407-41cb-9fb6-2f4a215f0aa3",
   "metadata": {},
   "source": [
    "### Discrete MO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d142d7-9882-4f79-bb00-1ff75c502927",
   "metadata": {},
   "source": [
    "set up and creats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04f126a4-432b-47b9-9ded-c21a6c269892",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############    Setting Environment\n",
    "e.reset()                             # reset the environment\n",
    "initial_observation=e.current_state   # make the initial signal from environment same as what actual \n",
    "                                      #state it has \n",
    "observation=initial_observation       # record the observation\n",
    "state=e.current_state                 # record the state\n",
    "observations=e.observations.keys()    # fetch the observations\n",
    "exp_num=len(exp_vorfaktoren)          # number of exponential terms\n",
    "num_of_actions=len(e.actions)         # number of actions\n",
    "num_of_observations=len(e.observations) #number of observations \n",
    "\n",
    "################ Making Agent\n",
    "\n",
    "############################## Discrete agent variables \n",
    "max_time_step=max_depth     # depth of planning\n",
    "r_round=5                   # number of decimal digits for rounding\n",
    "partitioning_points_num=10  # number of partitioning points: determines precision of belief estimation\n",
    "\n",
    "# Create the agent\n",
    "ag2=Multi_Variate_agent(environment=e ,max_time_step=max_time_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaabe40-ffbb-4e3f-a861-2552dbfabcef",
   "metadata": {},
   "source": [
    "value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28e8e327-eea5-450c-a6ff-aaa9147ddcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run its value iteration\n",
    "m_space_size,m_value_func,m_action_func,m_q_func=ag2.discrete_value_iteration(partitioning_points_num,initial_wealth,exp_weights,exp_vorfaktoren,rounding_prec_coeff=100000,\n",
    "                                 r_round=5,save_reachables=True,save_transition_kernel=True,use_buffer=True,\n",
    "                                 memory_threshold=1000000,keep_q_func=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70950b8-28b1-4d67-aadf-6e1b819dcbfa",
   "metadata": {},
   "source": [
    "simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5980937-d584-4d94-bcbd-3bf42019e209",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t= 0\n",
      "------\n",
      "state: 1 tiger_left\n",
      "last_observation: 1\n",
      "-------------------------------\n",
      "\n",
      "current internal state: ((0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1), (0, 0, 0), 0)\n",
      "\n",
      "             action: open_right_high\n",
      "             **\n",
      "reward: 0.4 new observation: 1\n",
      "\n",
      "new internal state: ((0.5, 0.5, 0.5, 0.5, 0.5, 0.5), (-0.07795, 0.008, 0.14538), 1)\n",
      "\n",
      "============================\n",
      "t= 1\n",
      "------\n",
      "state: 0 tiger_right\n",
      "last_observation: 1\n",
      "-------------------------------\n",
      "\n",
      "current internal state: ((0.5, 0.5, 0.5, 0.5, 0.5, 0.5), (-0.07795, 0.008, 0.14538), 1)\n",
      "\n",
      "             action: open_right_high\n",
      "             **\n",
      "reward: -0.4 new observation: 0\n",
      "\n",
      "new internal state: ((0.5, 0.5, 0.5, 0.5, 0.5, 0.5), (-0.1559, 0.016, 0.29076), 0)\n",
      "\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "# reset the agent \n",
    "ag2.reset(theta_0=theta_0,initial_observation=initial_observation,initial_wealth=initial_wealth)\n",
    "\n",
    "\n",
    "#############         Simulation\n",
    "\n",
    "for t in range(planning_depth):\n",
    "    \n",
    "    print('t=',t)\n",
    "    print('------')\n",
    "    print('state:',state,e.states[state])\n",
    "    print('last_observation:',observation)\n",
    "    print('-------------------------------')\n",
    "    print('')\n",
    "    print('current internal state:',ag2.current_internal_state)\n",
    "    print('')\n",
    "    \n",
    "    # agent select the action\n",
    "    action,value_of_action,_=ag2.do_action()\n",
    "    \n",
    "    #environment feedback\n",
    "    t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "    \n",
    "    # agent update\n",
    "    new_x=ag2.update_agent(new_observation=observation) \n",
    "    \n",
    "    print('             action:',num_to_act[action])  \n",
    "    print ('             **' )\n",
    "    print('reward:',reward,'new observation:',observation)\n",
    "    print('')\n",
    "    print('new internal state:', ag2.current_internal_state)\n",
    "    print('')\n",
    "    print('============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7242041-2e75-40e5-bade-f360fbe25508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
