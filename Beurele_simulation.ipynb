{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Version Guid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import agent as agent\n",
    "from env import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_depth=3\n",
    "num_of_Mu_chunks=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an instance of tiger porblem environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state_change_r2l': 0.1, 'state_change_l2r': 0.1, 'false_observation_get_l_while_r': 0.3, 'false_observation_get_r_while_l': 0.3, 'reward_listen': -0.5, 'reward_low_incorrect': -1, 'reward_high_incorrect': -2, 'reward_low_correct': 1, 'reward_high_correct': 2, 'discount_factor': 0.9, 'initial_wealth': 0}\n"
     ]
    }
   ],
   "source": [
    "e=tiger_POMDP_env(read_config=True,config_address='./tiger.json',parameters=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make an instance of Bauerle and Rieder agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag=agent.Bauerle_Rieder_agent(environment=e, num_of_Mu_chunks=num_of_Mu_chunks,max_iterations=maximum_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>value iteration</b><br>\n",
    "    1. Preparation of files and needed things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag.pre_planning_paration(make_and_save_Mu=True,save_Mu2index_chunks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.value-iteration<br>\n",
    "<br>\n",
    "Its result are in value_function, action_function, and step_indexes attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2\n",
      "==== iterations\n",
      "step: 1\n",
      "------\n",
      "step: 0\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "setting='d'+str(maximum_depth)+'c'+str(num_of_Mu_chunks)\n",
    "ag.value_iteration(utility_function='risk-neutral',save_results=False,load_results=True,depth_and_chunks=setting)\n",
    "#ag.value_iteration(utility_function=0.5,save_results=True,load_results=False,depth_and_chunks=setting) # risk-averse\n",
    "#ag.value_iteration(utility_function=-0.5,save_results=False,load_results=True,depth_and_chunks=setting) # risk-seeking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using value iteration, we have computed anything. So, we need to update our agent beliefs by interacting with environment.<br>\n",
    "We have two essential functions here: do_action() and update_agent() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An experiment with depth 2 and 8 chunk points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state_change_r2l': 0.1, 'state_change_l2r': 0.1, 'false_observation_get_l_while_r': 0.3, 'false_observation_get_r_while_l': 0.3, 'reward_listen': -0.5, 'reward_low_incorrect': -1, 'reward_high_incorrect': -2, 'reward_low_correct': 1, 'reward_high_correct': 2, 'discount_factor': 0.9, 'initial_wealth': 0}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import agent as agent\n",
    "from env import *\n",
    "\n",
    "################################################################ Simulation Parameters / rest are set in the config file\n",
    "maximum_depth=3\n",
    "num_of_Mu_chunks=3\n",
    "setting='d'+str(maximum_depth)+'c'+str(num_of_Mu_chunks)\n",
    "\n",
    "################################################################ Creating environment and the agent\n",
    "e=tiger_POMDP_env(read_config=True,config_address='./tiger.json',parameters=None)\n",
    "\n",
    "ag=agent.Bauerle_Rieder_agent(environment=e, num_of_Mu_chunks=num_of_Mu_chunks,max_iterations=maximum_depth)\n",
    "\n",
    "################################################################ Pre-planning\n",
    "t=datetime.datetime.now()\n",
    "ag.pre_planning_paration(make_and_save_Mu=True,save_Mu2index_chunks=True)\n",
    "print('Pre-planning:', datetime.datetime.now()-t)\n",
    "\n",
    "################################################################ value iteration\n",
    "t=datetime.datetime.now()\n",
    "\n",
    "#ag.value_iteration(utility_function='risk-neutral',save_results=True,load_results=False,depth_and_chunks=setting)\n",
    "#ag.value_iteration(utility_function=0.5,save_results=True,load_results=False,depth_and_chunks=setting) # risk-averse\n",
    "#ag.value_iteration(utility_function=-0.5,save_results=True,load_results=False,depth_and_chunks=setting) # risk-seeking\n",
    "\n",
    "print('Value iteration:', datetime.datetime.now()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Pre-planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (agent.py, line 1018)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\envs\\env3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3437\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-d8dcb3e4659e>\"\u001b[1;36m, line \u001b[1;32m5\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    import agent as agent\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"D:\\Vaios\\Work after fuck\\Code\\git\\MO-RS-POMDP\\agent.py\"\u001b[1;36m, line \u001b[1;32m1018\u001b[0m\n\u001b[1;33m    if self.current_internal_timeStep == self.\u001b[0m\n\u001b[1;37m                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import agent as agent\n",
    "from env import *\n",
    "\n",
    "################################################################ Simulation Parameters / rest are set in the config file\n",
    "maximum_depth=2\n",
    "num_of_Mu_chunks=4\n",
    "setting='d'+str(maximum_depth)+'c'+str(num_of_Mu_chunks)\n",
    "\n",
    "################################################################ Creating environment and the agent\n",
    "e=tiger_POMDP_env(read_config=True,config_address='./tiger.json',parameters=None)\n",
    "\n",
    "ag2=agent.Bauerle_Rieder_agent(environment=e, num_of_Mu_chunks=num_of_Mu_chunks,max_iterations=maximum_depth)\n",
    "\n",
    "################################################################ Pre-planning\n",
    "t=datetime.datetime.now()\n",
    "ag2.pre_planning_paration(make_and_save_Mu=False)\n",
    "print('Pre-planning:', datetime.datetime.now()-t)\n",
    "print(ag2.S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "ag2.value_iteration(utility_function='risk-neutral',save_results=True,load_results=False,depth_and_chunks=setting) # risk-Neutral\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full simulation\n",
    "It contains Pre-planning and Value-iteration as well.  So, just run the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> New Risk Neutral\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state_change_r2l': 0.1, 'state_change_l2r': 0.1, 'false_observation_get_l_while_r': 0.3, 'false_observation_get_r_while_l': 0.3, 'reward_listen': -0.5, 'reward_low_incorrect': -1, 'reward_high_incorrect': -2, 'reward_low_correct': 1, 'reward_high_correct': 2, 'discount_factor': 0.9, 'initial_wealth': 0}\n",
      "Pre-planning: 0:00:00.001005\n",
      "{0: [0], 1: [-2.0, -1.0, -0.5, 1.0, 2.0], 2: [-3.8, -2.9, -2.8, -2.45, -2.3, -1.9, -1.45, -1.4, -1.1, -0.95, -0.8, -0.19999999999999996, -0.09999999999999998, 0.09999999999999998, 0.19999999999999996, 0.4, 0.55, 0.8, 1.1, 1.3, 1.55, 1.9, 2.8, 2.9, 3.8]}\n",
      "step: 2\n",
      "==== iterations\n",
      "step: 1\n",
      "------\n",
      "step: 0\n",
      "------\n",
      "initial beliefs:\n",
      "[[0. ]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "initial internal state(observation): 1\n",
      "========================= Simulation satrts =====================\n",
      "====================================================\n",
      "t= 0\n",
      "------\n",
      "state: 1 tiger_left\n",
      "observation: 1\n",
      "             belief_at_action:\n",
      "[[0. ]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "\n",
      "             action: listen\n",
      "              **\n",
      "reward: -0.5 new observation: 1\n",
      "\n",
      "            new_beliefs:\n",
      "[[-2.   -1.   -0.5   1.    2.  ]\n",
      " [ 0.    0.    0.25  0.    0.  ]\n",
      " [ 0.    0.    0.75  0.    0.  ]]\n",
      "====================================================\n",
      "====================================================\n",
      "t= 1\n",
      "------\n",
      "state: 1 tiger_left\n",
      "observation: 1\n",
      "             belief_at_action:\n",
      "[[-2.   -1.   -0.5   1.    2.  ]\n",
      " [ 0.    0.    0.25  0.    0.  ]\n",
      " [ 0.    0.    0.75  0.    0.  ]]\n",
      "\n",
      "             action: open_right_high\n",
      "              **\n",
      "reward: 2.0 new observation: 0\n",
      "\n",
      "            new_beliefs:\n",
      "[[-3.8  -2.9  -2.8  -2.45 -2.3  -1.9  -1.45 -1.4  -1.1  -0.95 -0.8  -0.2\n",
      "  -0.1   0.1   0.2   0.4   0.55  0.8   1.1   1.3   1.55  1.9   2.8   2.9\n",
      "   3.8 ]\n",
      " [ 0.    0.    0.    0.    0.25  0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.5   0.    0.    0.    0.\n",
      "   0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.25  0.    0.    0.    0.\n",
      "   0.  ]]\n",
      "====================================================\n",
      "Value-iteration: 0:01:29.899126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\env3\\lib\\site-packages\\numpy\\lib\\npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import agent as agent\n",
    "from env import *\n",
    "\n",
    "################################################################ Simulation Parameters / rest are set in the config file\n",
    "maximum_depth=2\n",
    "num_of_Mu_chunks=4\n",
    "setting='d'+str(maximum_depth)+'c'+str(num_of_Mu_chunks)\n",
    "\n",
    "################################################################ Creating environment and the agent\n",
    "e=tiger_POMDP_env(read_config=True,config_address='./tiger.json',parameters=None)\n",
    "\n",
    "ag2=agent.Bauerle_Rieder_agent(environment=e, num_of_Mu_chunks=num_of_Mu_chunks,max_iterations=maximum_depth)\n",
    "\n",
    "################################################################ Pre-planning\n",
    "t=datetime.datetime.now()\n",
    "ag2.pre_planning_paration(make_and_save_Mu=False)\n",
    "print('Pre-planning:', datetime.datetime.now()-t)\n",
    "print(ag2.S)\n",
    "\n",
    "############################################################### value iteration\n",
    "\n",
    "starting_time=datetime.datetime.now()\n",
    "ag2.value_iteration(utility_function='risk-neutral',save_results=True,load_results=False,depth_and_chunks=setting) # risk-Neutral\n",
    "num_to_act=dict(zip(list(e.actions.values()),list(e.actions.keys())))\n",
    "\n",
    "################################################################## Simulation\n",
    "\n",
    "# reset the agent with random starting x and probability of P(y=0 and s=0)=1/3 and P(y=1 and s=0)=2/3\n",
    "e.reset()\n",
    "initial_observation=e.current_state\n",
    "ag2.reset(y0_prob=0.5,initiative_observation=initial_observation)\n",
    "observation=initial_observation\n",
    "state=e.current_state\n",
    "print('========================= Simulation satrts =====================')\n",
    "for t in range(maximum_depth):\n",
    "    print('====================================================')\n",
    "    print('t=',t)\n",
    "    print('------')\n",
    "    print('state:',state,e.states[state])\n",
    "    print('observation:',observation) \n",
    "    \n",
    "    action,value,belief_at_action=ag2.do_action()   \n",
    "    t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "    new_mu=ag2.update_agent(new_observation=observation)\n",
    "    \n",
    "    print('             belief_at_action:')   \n",
    "    print(belief_at_action)\n",
    "    \n",
    "    #print('value_at_action:')\n",
    "    #print(value)\n",
    "    print('')\n",
    "    print('             action:',num_to_act[action])  \n",
    "    \n",
    "    \n",
    "    print ('              **' )\n",
    "    print('reward:',reward,'new observation:',observation)\n",
    "    print('')\n",
    "    print('            new_beliefs:')\n",
    "    print (new_mu)\n",
    "    print('====================================================')\n",
    "print('Value-iteration:', datetime.datetime.now()-starting_time)    \n",
    "#print('state:',e.current_state,'observation:',ag.current_internal_x )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> New Risk Averse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state_change_r2l': 0.1, 'state_change_l2r': 0.1, 'false_observation_get_l_while_r': 0.3, 'false_observation_get_r_while_l': 0.3, 'reward_listen': -0.5, 'reward_low_incorrect': -1, 'reward_high_incorrect': -2, 'reward_low_correct': 1, 'reward_high_correct': 2, 'discount_factor': 0.9, 'initial_wealth': 0}\n",
      "Pre-planning: 0:00:00.001992\n",
      "{0: [0], 1: [-2.0, -1.0, -0.5, 1.0, 2.0], 2: [-3.8, -2.9, -2.8, -2.45, -2.3, -1.9, -1.45, -1.4, -1.1, -0.95, -0.8, -0.19999999999999996, -0.09999999999999998, 0.09999999999999998, 0.19999999999999996, 0.4, 0.55, 0.8, 1.1, 1.3, 1.55, 1.9, 2.8, 2.9, 3.8]}\n",
      "step: 2\n",
      "==== iterations\n",
      "step: 1\n",
      "------\n",
      "step: 0\n",
      "------\n",
      "initial beliefs:\n",
      "[[0. ]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "initial internal state(observation): 1\n",
      "========================= Simulation satrts =====================\n",
      "====================================================\n",
      "t= 0\n",
      "------\n",
      "state: 1 tiger_left\n",
      "observation: 1\n",
      "             belief_at_action:\n",
      "[[0. ]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "\n",
      "             action: listen\n",
      "              **\n",
      "reward: -0.5 new observation: 1\n",
      "\n",
      "            new_beliefs:\n",
      "[[-2.   -1.   -0.5   1.    2.  ]\n",
      " [ 0.    0.    0.25  0.    0.  ]\n",
      " [ 0.    0.    0.75  0.    0.  ]]\n",
      "====================================================\n",
      "====================================================\n",
      "t= 1\n",
      "------\n",
      "state: 1 tiger_left\n",
      "observation: 1\n",
      "             belief_at_action:\n",
      "[[-2.   -1.   -0.5   1.    2.  ]\n",
      " [ 0.    0.    0.25  0.    0.  ]\n",
      " [ 0.    0.    0.75  0.    0.  ]]\n",
      "\n",
      "             action: open_right_low\n",
      "              **\n",
      "reward: 1.0 new observation: 1\n",
      "\n",
      "            new_beliefs:\n",
      "[[-3.8  -2.9  -2.8  -2.45 -2.3  -1.9  -1.45 -1.4  -1.1  -0.95 -0.8  -0.2\n",
      "  -0.1   0.1   0.2   0.4   0.55  0.8   1.1   1.3   1.55  1.9   2.8   2.9\n",
      "   3.8 ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.25  0.    0.    0.    0.\n",
      "   0.    0.    0.    0.5   0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.25  0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.  ]]\n",
      "====================================================\n",
      "Value-iteration: 0:01:24.403200\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import agent as agent\n",
    "from env import *\n",
    "\n",
    "################################################################ Simulation Parameters / rest are set in the config file\n",
    "maximum_depth=2\n",
    "num_of_Mu_chunks=4\n",
    "setting='d'+str(maximum_depth)+'c'+str(num_of_Mu_chunks)\n",
    "\n",
    "################################################################ Creating environment and the agent\n",
    "e=tiger_POMDP_env(read_config=True,config_address='./tiger.json',parameters=None)\n",
    "\n",
    "ag2=agent.Bauerle_Rieder_agent(environment=e, num_of_Mu_chunks=num_of_Mu_chunks,max_iterations=maximum_depth)\n",
    "\n",
    "################################################################ Pre-planning\n",
    "t=datetime.datetime.now()\n",
    "ag2.pre_planning_paration(make_and_save_Mu=False)\n",
    "print('Pre-planning:', datetime.datetime.now()-t)\n",
    "print(ag2.S)\n",
    "################################################################ value iteration\n",
    "starting_time=datetime.datetime.now()\n",
    "ag2.value_iteration(utility_function=1,save_results=True,load_results=False,depth_and_chunks=setting) # risk-averse\n",
    "num_to_act=dict(zip(list(e.actions.values()),list(e.actions.keys())))\n",
    "\n",
    "################################################################# simulation\n",
    "\n",
    "# reset the agent with random starting x and probability of P(y=0 and s=0)=1/3 and P(y=1 and s=0)=2/3\n",
    "e.reset()\n",
    "initial_observation=e.current_state\n",
    "ag2.reset(y0_prob=0.5,initiative_observation=initial_observation)\n",
    "observation=initial_observation\n",
    "state=e.current_state\n",
    "print('========================= Simulation satrts =====================')\n",
    "for t in range(maximum_depth):\n",
    "    print('====================================================')\n",
    "    print('t=',t)\n",
    "    print('------')\n",
    "    print('state:',state,e.states[state])\n",
    "    print('observation:',observation) \n",
    "    \n",
    "    action,value,belief_at_action=ag2.do_action()   \n",
    "    t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "    new_mu=ag2.update_agent(new_observation=observation)\n",
    "    \n",
    "    print('             belief_at_action:')   \n",
    "    print(belief_at_action)\n",
    "    \n",
    "    #print('value_at_action:')\n",
    "    #print(value)\n",
    "    print('')\n",
    "    print('             action:',num_to_act[action])  \n",
    "    \n",
    "    \n",
    "    print ('              **' )\n",
    "    print('reward:',reward,'new observation:',observation)\n",
    "    print('')\n",
    "    print('            new_beliefs:')\n",
    "    print (new_mu)\n",
    "    print('====================================================')\n",
    "print('Value-iteration:', datetime.datetime.now()-starting_time)    \n",
    "#print('state:',e.current_state,'observation:',ag.current_internal_x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> New Risk Seeking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state_change_r2l': 0.1, 'state_change_l2r': 0.1, 'false_observation_get_l_while_r': 0.3, 'false_observation_get_r_while_l': 0.3, 'reward_listen': -0.5, 'reward_low_incorrect': -1, 'reward_high_incorrect': -2, 'reward_low_correct': 1, 'reward_high_correct': 2, 'discount_factor': 0.9, 'initial_wealth': 0}\n",
      "Pre-planning: 0:00:00.004987\n",
      "{0: [0], 1: [-2.0, -1.0, -0.5, 1.0, 2.0], 2: [-3.8, -2.9, -2.8, -2.45, -2.3, -1.9, -1.45, -1.4, -1.1, -0.95, -0.8, -0.19999999999999996, -0.09999999999999998, 0.09999999999999998, 0.19999999999999996, 0.4, 0.55, 0.8, 1.1, 1.3, 1.55, 1.9, 2.8, 2.9, 3.8]}\n",
      "step: 2\n",
      "==== iterations\n",
      "step: 1\n",
      "------\n",
      "step: 0\n",
      "------\n",
      "initial beliefs:\n",
      "[[0. ]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "initial internal state(observation): 1\n",
      "========================= Simulation satrts =====================\n",
      "====================================================\n",
      "t= 0\n",
      "------\n",
      "state: 1 tiger_left\n",
      "observation: 1\n",
      "             belief_at_action:\n",
      "[[0. ]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "\n",
      "             action: open_right_high\n",
      "              **\n",
      "reward: 2.0 new observation: 1\n",
      "\n",
      "            new_beliefs:\n",
      "[[-2.   -1.   -0.5   1.    2.  ]\n",
      " [ 0.25  0.    0.    0.    0.25]\n",
      " [ 0.25  0.    0.    0.    0.25]]\n",
      "====================================================\n",
      "====================================================\n",
      "t= 1\n",
      "------\n",
      "state: 0 tiger_right\n",
      "observation: 1\n",
      "             belief_at_action:\n",
      "[[-2.   -1.   -0.5   1.    2.  ]\n",
      " [ 0.25  0.    0.    0.    0.25]\n",
      " [ 0.25  0.    0.    0.    0.25]]\n",
      "\n",
      "             action: open_right_high\n",
      "              **\n",
      "reward: -2.0 new observation: 0\n",
      "\n",
      "            new_beliefs:\n",
      "[[-3.8  -2.9  -2.8  -2.45 -2.3  -1.9  -1.45 -1.4  -1.1  -0.95 -0.8  -0.2\n",
      "  -0.1   0.1   0.2   0.4   0.55  0.8   1.1   1.3   1.55  1.9   2.8   2.9\n",
      "   3.8 ]\n",
      " [ 0.25  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.25\n",
      "   0.    0.    0.25  0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.25]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.  ]]\n",
      "====================================================\n",
      "Value-iteration: 0:01:25.420964\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import agent as agent\n",
    "from env import *\n",
    "\n",
    "################################################################ Simulation Parameters / rest are set in the config file\n",
    "maximum_depth=2\n",
    "num_of_Mu_chunks=4\n",
    "setting='d'+str(maximum_depth)+'c'+str(num_of_Mu_chunks)\n",
    "\n",
    "################################################################ Creating environment and the agent\n",
    "e=tiger_POMDP_env(read_config=True,config_address='./tiger.json',parameters=None)\n",
    "\n",
    "ag2=agent.Bauerle_Rieder_agent(environment=e, num_of_Mu_chunks=num_of_Mu_chunks,max_iterations=maximum_depth)\n",
    "\n",
    "################################################################ Pre-planning\n",
    "t=datetime.datetime.now()\n",
    "ag2.pre_planning_paration(make_and_save_Mu=False)\n",
    "print('Pre-planning:', datetime.datetime.now()-t)\n",
    "print(ag2.S)\n",
    "################################################################# value iteration\n",
    "starting_time=datetime.datetime.now()\n",
    "ag2.value_iteration(utility_function=-1,save_results=True,load_results=False,depth_and_chunks=setting) # risk-Seeker\n",
    "num_to_act=dict(zip(list(e.actions.values()),list(e.actions.keys())))\n",
    "\n",
    "# reset the agent with random starting x and probability of P(y=0 and s=0)=1/3 and P(y=1 and s=0)=2/3\n",
    "e.reset()\n",
    "initial_observation=e.current_state\n",
    "ag2.reset(y0_prob=0.5,initiative_observation=initial_observation)\n",
    "observation=initial_observation\n",
    "state=e.current_state\n",
    "print('========================= Simulation satrts =====================')\n",
    "for t in range(maximum_depth):\n",
    "    print('====================================================')\n",
    "    print('t=',t)\n",
    "    print('------')\n",
    "    print('state:',state,e.states[state])\n",
    "    print('observation:',observation) \n",
    "    \n",
    "    action,value,belief_at_action=ag2.do_action()   \n",
    "    t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "    new_mu=ag2.update_agent(new_observation=observation)\n",
    "    \n",
    "    print('             belief_at_action:')   \n",
    "    print(belief_at_action)\n",
    "    \n",
    "    #print('value_at_action:')\n",
    "    #print(value)\n",
    "    print('')\n",
    "    print('             action:',num_to_act[action])  \n",
    "    \n",
    "    \n",
    "    print ('              **' )\n",
    "    print('reward:',reward,'new observation:',observation)\n",
    "    print('')\n",
    "    print('            new_beliefs:')\n",
    "    print (new_mu)\n",
    "    print('====================================================')\n",
    "print('Value-iteration:', datetime.datetime.now()-starting_time)    \n",
    "#print('state:',e.current_state,'observation:',ag.current_internal_x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
