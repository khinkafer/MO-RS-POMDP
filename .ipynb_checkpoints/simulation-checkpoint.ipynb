{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import agent as agent\n",
    "from env import *\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_depth=3\n",
    "num_of_Mu_chunks=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an instance of tiger porblem environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "e=tiger_POMDP_env(read_config=True,config_address='./tiger.json',parameters=None)\n",
    "e.discount_factor = 1\n",
    "for i in range(1,len(e.actions)):\n",
    "    e.observation_matrix[i][0][0]=0.5\n",
    "    e.observation_matrix[i][0][1]=0.5\n",
    "    e.observation_matrix[i][1][0]=0.5\n",
    "    e.observation_matrix[i][1][1]=0.5\n",
    "\n",
    "# listening cost\n",
    "e.rewards[:,0]=-1.\n",
    "# correct and low\n",
    "e.rewards[0,1]=e.rewards[1,2]=10.\n",
    "# incorrect and low\n",
    "e.rewards[0,2]=e.rewards[1,1]=-100.\n",
    "# correct and high\n",
    "e.rewards[0,3]=e.rewards[1,4]=20.\n",
    "# incorrect and high\n",
    "e.rewards[0,4]=e.rewards[1,3]=-200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make an instance of Bauerle and Rieder agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag=agent.Bauerle_Rieder_agent(environment=e, num_of_Mu_chunks=num_of_Mu_chunks,max_iterations=maximum_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEMP VIS\n",
    "get_possible_s = lambda step : ag.generate_possible_wealths(np.unique(ag.env.rewards),ag.initial_wealth,ag.env.discount_factor,step)\n",
    "def vis_belief_state(belief,title=\"Belief\"):\n",
    "    wealth = list(get_possible_s(maximum_depth))\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(25, 5))\n",
    "    fig.suptitle(title)\n",
    "    ax1.set_title(\"Left\")\n",
    "    ax1.bar(list(np.arange(1,len(wealth)+1)), list(belief[0:len(wealth)]), width=1.0)\n",
    "    ax1.set_xticks(list(np.arange(1,len(wealth)+1)))\n",
    "    ax1.set_xticklabels([str(i) for i in wealth])\n",
    "    ax1.set_ylim(0,num_of_Mu_chunks+1)\n",
    "    ax2.set_title(\"Right\")\n",
    "    ax2.bar(list(np.arange(1,len(wealth)+1)), list(belief[len(wealth):len(wealth)*2]), width=1.0)\n",
    "    ax2.set_xticks(list(np.arange(1,len(wealth)+1)))\n",
    "    ax2.set_xticklabels([str(i) for i in wealth])\n",
    "    ax2.set_ylim(0,num_of_Mu_chunks+1)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 66/3916.0 [00:00<00:05, 659.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation Possible Wealths\n",
      "Length S 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7657it [00:05, 1293.46it/s]                            \n",
      " 17%|█▋        | 1/6 [00:00<00:00,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  6.19it/s]\n"
     ]
    }
   ],
   "source": [
    "ag.pre_planning_paration(make_and_save_Mu=True,save_Mu2index_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3\n",
      "==== iterations\n",
      "step: 2\n",
      "a: 0\n",
      "a: 1\n",
      "a: 2\n",
      "a: 3\n",
      "a: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 2/4 [00:47<00:47, 23.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "step: 1\n",
      "a: 0\n",
      "a: 1\n",
      "a: 2\n",
      "a: 3\n",
      "a: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 3/4 [01:34<00:30, 30.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "step: 0\n",
      "a: 0\n"
     ]
    }
   ],
   "source": [
    "ident = lambda x: x\n",
    "exp = lambda x: np.exp(x)\n",
    "xa = ag.value_iteration(utility_function=ident)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.value-iteration<br>\n",
    "<br>\n",
    "Its result are in value_function, action_function, and step_indexes attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using value iteration, we have computed anything. So, we need to update our agent beliefs by interacting with environment.<br>\n",
    "We have two essential functions here: do_action() and update_agent() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An experiment with depth 2 and 3 chunk points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_act=dict(zip(list(e.actions.values()),list(e.actions.keys())))\n",
    "\n",
    "# reset the agent with random starting x and probability of P(y=0 and s=0)=1/3 and P(y=1 and s=0)=2/3\n",
    "ag.reset()\n",
    "\n",
    "max_s = list(get_possible_s(maximum_depth))*2\n",
    "acc_reward = 0\n",
    "\n",
    "vis_belief_state(ag.current_internal_belief,title=\"Init Belief\")\n",
    "\n",
    "for t in range(maximum_depth):\n",
    "    action,value,belief=ag.do_action()   \n",
    "    print(\"Agent: Action={} Value: {} True State: {} \".format(num_to_act[action],value,e.current_state))\n",
    "    \n",
    "    t1,t2,new_state,reward,new_observation=e.step(num_to_act[action])\n",
    "    print(\"EnvStep: NewState={} Observation={} Reward={}\".format(new_state,new_observation,reward))\n",
    "    \n",
    "    acc_reward += reward\n",
    "    \n",
    "    ag.update_agent(new_observation)\n",
    "\n",
    "    #print('Step: {} New State: {} New Observation: {} New Belief {}'.format(ag.current_internal_timeStep,e.current_state,new_observation,np.argmax(ag.current_internal_belief)))\n",
    "    #print(ag.current_internal_belief)\n",
    "    vis_belief_state(ag.current_internal_belief[0,:],title=\"Belief \"+str(t))\n",
    "    \n",
    "print(\"Actual Reward {}, Beliefed Reward: {}\".format(acc_reward,max_s[np.argmax(ag.current_internal_belief)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%TODO\n",
    "##TEST BELIEF UPDATE\n",
    "\n",
    "#[0 0 0 2 0 0 0 0 0 1 0 0]\n",
    "belief=np.zeros(shape=(1,38))\n",
    "#left\n",
    "belief[0,12]=2\n",
    "belief[0,12+19]=2\n",
    "vis_belief_state(belief[0,:])\n",
    "\n",
    "step = 0\n",
    "obs = 1\n",
    "print(obs)\n",
    "result=ag.say_calculator(x=0,\n",
    "                         action=0,\n",
    "                         x_prim=obs,\n",
    "                         current_int_mu=belief,\n",
    "                         z=np.power(ag.env.gamma,step),\n",
    "                         current_possible_s=get_possible_s(step))\n",
    "vis_belief_state(result[0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
