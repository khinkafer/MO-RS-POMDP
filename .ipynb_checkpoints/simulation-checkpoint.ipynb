{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import agent as agent\n",
    "from env import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_depth=2\n",
    "num_of_Mu_chunks=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an instance of tiger porblem environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state_change_r2l': 0.1, 'state_change_l2r': 0.1, 'false_observation_get_l_while_r': 0.3, 'false_observation_get_r_while_l': 0.3, 'reward_listen': -0.5, 'reward_low_incorrect': -1, 'reward_high_incorrect': -2, 'reward_low_correct': 1, 'reward_high_correct': 2, 'discount_factor': 1, 'initial_wealth': 0}\n"
     ]
    }
   ],
   "source": [
    "e=tiger_POMDP_env(read_config=True,config_address='./tiger.json',parameters=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make an instance of Bauerle and Rieder agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag=agent.Bauerle_Rieder_agent(environment=e, num_of_Mu_chunks=num_of_Mu_chunks,max_iterations=maximum_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>value iteration</b><br>\n",
    "    1. Preparation of files and needed things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag.pre_planning_paration(make_and_save_Mu=True,save_Mu2index_chunks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.value-iteration<br>\n",
    "<br>\n",
    "Its result are in value_function, action_function, and step_indexes attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2\n",
      "==== iterations\n",
      "step: 1\n",
      "------\n",
      "step: 0\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "ag.value_iteration(utility_function='risk-neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using value iteration, we have computed anything. So, we need to update our agent beliefs by interacting with environment.<br>\n",
    "We have two essential functions here: do_action() and update_agent() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An experiment with depth 2 and 5 chunk points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state_change_r2l': 0.1, 'state_change_l2r': 0.1, 'false_observation_get_l_while_r': 0.3, 'false_observation_get_r_while_l': 0.3, 'reward_listen': -0.5, 'reward_low_incorrect': -1, 'reward_high_incorrect': -2, 'reward_low_correct': 1, 'reward_high_correct': 2, 'discount_factor': 1, 'initial_wealth': 0}\n",
      "step: 3\n",
      "==== iterations\n",
      "step: 2\n",
      "------\n",
      "step: 1\n",
      "------\n",
      "step: 0\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import agent as agent\n",
    "from env import *\n",
    "\n",
    "maximum_depth=3\n",
    "num_of_Mu_chunks=4\n",
    "\n",
    "e=tiger_POMDP_env(read_config=True,config_address='./tiger.json',parameters=None)\n",
    "ag=agent.Bauerle_Rieder_agent(environment=e, num_of_Mu_chunks=num_of_Mu_chunks,max_iterations=maximum_depth)\n",
    "ag.pre_planning_paration(make_and_save_Mu=True,save_Mu2index_chunks=True)\n",
    "ag.value_iteration(utility_function='risk-neutral')\n",
    "#ag.value_iteration(utility_function=0.5) # risk-averse\n",
    "#ag.value_iteration(utility_function=-0.5) # risk-seeking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial beliefs:\n",
      "[[-6.  -5.  -4.5 -4.  -3.5 -3.  -2.5 -2.  -1.5 -1.  -0.5  0.   0.5  1.\n",
      "   1.5  2.   2.5  3.   3.5  4.   5.   6. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.5  0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.5  0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. ]]\n",
      "initial internal state(observation): 0\n",
      "t= 0\n",
      "------\n",
      "state: 0 tiger_right\n",
      "observation: 0\n",
      "belief_at_action:\n",
      "[[-6.  -5.  -4.5 -4.  -3.5 -3.  -2.5 -2.  -1.5 -1.  -0.5  0.   0.5  1.\n",
      "   1.5  2.   2.5  3.   3.5  4.   5.   6. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.5  0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.5  0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. ]]\n",
      "value_at_action:\n",
      "2.0\n",
      "action: open_right_low\n",
      "**\n",
      "reward: -1.0\n",
      "new observation: 0\n",
      "new_beliefs:\n",
      "[[-6.   -5.   -4.5  -4.   -3.5  -3.   -2.5  -2.   -1.5  -1.   -0.5   0.\n",
      "   0.5   1.    1.5   2.    2.5   3.    3.5   4.    5.    6.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.25  0.    0.\n",
      "   0.    0.25  0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.25  0.    0.\n",
      "   0.    0.25  0.    0.    0.    0.    0.    0.    0.    0.  ]]\n",
      "=======================\n",
      "t= 1\n",
      "------\n",
      "state: 1 tiger_left\n",
      "observation: 0\n",
      "belief_at_action:\n",
      "[[-6.   -5.   -4.5  -4.   -3.5  -3.   -2.5  -2.   -1.5  -1.   -0.5   0.\n",
      "   0.5   1.    1.5   2.    2.5   3.    3.5   4.    5.    6.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.25  0.    0.\n",
      "   0.    0.25  0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.25  0.    0.\n",
      "   0.    0.25  0.    0.    0.    0.    0.    0.    0.    0.  ]]\n",
      "value_at_action:\n",
      "2.0\n",
      "action: open_right_high\n",
      "**\n",
      "reward: 2.0\n",
      "new observation: 0\n",
      "new_beliefs:\n",
      "[[-6.   -5.   -4.5  -4.   -3.5  -3.   -2.5  -2.   -1.5  -1.   -0.5   0.\n",
      "   0.5   1.    1.5   2.    2.5   3.    3.5   4.    5.    6.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.25  0.    0.    0.    0.25  0.    0.\n",
      "   0.    0.25  0.    0.    0.    0.25  0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n",
      "=======================\n",
      "t= 2\n",
      "------\n",
      "state: 1 tiger_left\n",
      "observation: 0\n",
      "belief_at_action:\n",
      "[[-6.   -5.   -4.5  -4.   -3.5  -3.   -2.5  -2.   -1.5  -1.   -0.5   0.\n",
      "   0.5   1.    1.5   2.    2.5   3.    3.5   4.    5.    6.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.25  0.    0.    0.    0.25  0.    0.\n",
      "   0.    0.25  0.    0.    0.    0.25  0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n",
      "value_at_action:\n",
      "2.0\n",
      "action: open_left_high\n",
      "**\n",
      "reward: -2.0\n",
      "new observation: 0\n",
      "new_beliefs:\n",
      "[[-6.   -5.   -4.5  -4.   -3.5  -3.   -2.5  -2.   -1.5  -1.   -0.5   0.\n",
      "   0.5   1.    1.5   2.    2.5   3.    3.5   4.    5.    6.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.25  0.    0.\n",
      "   0.    0.25  0.    0.    0.    0.25  0.    0.    0.25  0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_to_act=dict(zip(list(e.actions.values()),list(e.actions.keys())))\n",
    "\n",
    "# reset the agent with random starting x and probability of P(y=0 and s=0)=1/3 and P(y=1 and s=0)=2/3\n",
    "e.reset()\n",
    "initial_observation=e.current_state\n",
    "ag.reset(y0_prob=0.5,initiative_observation=initial_observation)\n",
    "observation=initial_observation\n",
    "state=e.current_state\n",
    "for t in range(maximum_depth):\n",
    "    \n",
    "    print('t=',t)\n",
    "    print('------')\n",
    "    print('state:',state,e.states[state])\n",
    "    print('observation:',observation) \n",
    "    \n",
    "    action,value,belief_at_action=ag.do_action()   \n",
    "    t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "    new_mu=ag.update_agent(new_observation=observation)\n",
    "    \n",
    "    print('belief_at_action:')   \n",
    "    print(belief_at_action)\n",
    "    \n",
    "    print('value_at_action:')\n",
    "    print(value)\n",
    "    \n",
    "    print('action:',num_to_act[action])  \n",
    "    \n",
    "    \n",
    "    print ('**' )\n",
    "    print('reward:',reward)\n",
    "    print('new observation:',observation)\n",
    "    print('new_beliefs:')\n",
    "    print (new_mu)\n",
    "    print('=======================')\n",
    "    \n",
    "    #print('state:',e.current_state,'observation:',ag.current_internal_x )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
